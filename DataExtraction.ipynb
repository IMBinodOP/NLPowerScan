{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 42608.6.txt saved successfully.\n",
      "File 43070.4.txt saved successfully.\n",
      "File 43532.2.txt saved successfully.\n",
      "File 43994.0.txt saved successfully.\n",
      "File 44455.8.txt saved successfully.\n",
      "File 44917.6.txt saved successfully.\n",
      "File 45379.4.txt saved successfully.\n",
      "File 45841.2.txt saved successfully.\n",
      "File 46303.0.txt saved successfully.\n",
      "File 46764.8.txt saved successfully.\n",
      "File 47226.6.txt saved successfully.\n",
      "File 47688.4.txt saved successfully.\n",
      "File 48150.2.txt saved successfully.\n",
      "File 48612.0.txt saved successfully.\n",
      "File 49073.8.txt saved successfully.\n",
      "File 49535.6.txt saved successfully.\n",
      "File 49997.4.txt saved successfully.\n",
      "File 50459.2.txt saved successfully.\n",
      "File 50921.0.txt saved successfully.\n",
      "File 51382.8.txt saved successfully.\n",
      "File 51844.6.txt saved successfully.\n",
      "File 52306.4.txt saved successfully.\n",
      "File 52768.2.txt saved successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# Read the Excel file\n",
    "df = pd.read_excel('input.xlsx')\n",
    "\n",
    "# Define a function to extract text from a URL and save it to a file\n",
    "def extract_text_and_save(url_id, url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Extracting the article text assuming it's within <p> tags\n",
    "        article_text = '\\n'.join([p.get_text() for p in soup.find_all('p')])\n",
    "        with open(f\"{url_id}.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "            file.write(article_text)\n",
    "        print(f\"File {url_id}.txt saved successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred for {url_id}: {e}\")\n",
    "\n",
    "# Iterate over the dataframe and extract text for each URL\n",
    "for index, row in df.iterrows():\n",
    "    url_id = row['URL_ID']\n",
    "    url = row['URL']\n",
    "    extract_text_and_save(url_id, url)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
